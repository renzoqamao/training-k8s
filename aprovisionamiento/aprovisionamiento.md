# Tipos de aprovisionamiento

## Aprovisionamiento est√°tico

En el aprovisionamiento est√°tico, los administradores crean previamente los `PersistentVolume (PV)` antes de que los pods los soliciten mediante un `PersistenceVolumeClaim (PVC)`. Estos vol√∫menes tienen `tama√±o fijo` y est√°n vinculados a un recurso de almacenamiento ya existente. 

### HostPath

Solo para entornos locales o pruebas. No es recomendable para producci√≥n ya que esta ligado √∫nicamente a un √∫nico nodo del cluster.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-static-hostpath
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-static-hostpath
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

```

En este caso el administrador debe crear `/mnt/data` en el nodo manualmente.

### NFS (Network file System)

En este caso los pods se conectan a un servidor NFS externo. Permite que varios pods y nodos accedan al mismo volumen (ReadWriteMany). Es muy usado en cl√∫steres locales y on-premise. Es necesario que halla un servidor nfs externo disponible que este expuesto a la interfaz de red del cl√∫ster y exporter `/export/data`.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-static-nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.100   # IP del servidor NFS
    path: "/export/data"
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-static-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
```
### iSCSI (Internet Small Computer System Interface)

Se conecta a un dispositivo de almacenamiento via protocolo iSCSI. Permite bloques de disco remoto como si fueran locales. Utilizado en entornos empresariales

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-static-iscsi
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  iscsi:
    targetPortal: 10.20.30.40:3260
    iqn: iqn.2001-04.com.example:storage.lun1
    lun: 0
    fsType: ext4
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-static-iscsi
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
En este caso el administrador ya tiene configurado el servidor iSCSI con el target y el LUN.

### Volumenes tipo Cloud

En la nube se pueden utilizar vol√∫menes est√°ticos dados por el proveedor. En AWS se puede utilizar EBS.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-static-ebs
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-0a1b2c3d4e5f67890   # ID creado manualmente en AWS
    fsType: ext4
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-static-ebs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi

```

En este caso el Elastic Block Storage debe crearse manualmente previamente.

## Aprovisionamiento din√°mico

### Local Path Provisioner

Crea directorios locales en nodos bajo demanda. Utilizado en desarrollo, cada vez que se crea un PVC, genera un carpeta en el nodo `(ej. /opt/local-path-provisioner/pvc-xxx)`.

Creaci√≥n del StorageClass(Provisioner):

```yaml
# (Ejemplo de StorageClass para Local Path Provisioner)  localPathProvisioner.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  pathPattern: "/opt/local-path-provisioner/%s"
```

```bash
kubectl apply -f localPathProvisioner.yaml
```

Despliegue r√°pido con el manifiesto oficial:

```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
kubectl get storageclass
kubectl describe storageclass local-path
```

Ejemplo de PVC que reclama un volumen:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-local-path
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path   # StorageClass creada por Local Path Provisioner
  resources:
    requests:
      storage: 2Gi

```

> Notas importantes:
> - volumeBindingMode: WaitForFirstConsumer es recomendable en local: evita crear el volumen en un nodo distinto al que finalmente ejecutar√° el Pod.
> - Los vol√∫menes son locales al nodo; si el pod se mueve a otro nodo, el dato no lo acompa√±a (no es HA por s√≠ solo). Ideal para dev/test.

### NFS Subdir External Provisioner

Usa un servidor NFS existente. Crea automaticamente subdirectorios en el servidor NFS cada que vez que un PVC lo solicita, evitando configuraciones manuales de PV. Es √∫til para compartir vol√∫menes entre nodos.

La manera m√°s r√°pida de desplegar el NFS Subdir External provisioner es utilizando helm:

```bash
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm repo update
helm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --namespace kube-system --create-namespace -f nfs-values.yaml
```

```yaml 
# nfs-values.yaml
replicaCount: 1

nfs:
  server: "192.168.1.100"   # <-- Cambia por la IP/hostname de tu NFS
  path: "/exported/path"    # <-- Cambia por el path exportado

provisioner:
  # nombre del provisioner que se usar√° en la StorageClass
  name: "example.com/nfs"

storageClass:
  create: true
  name: "nfs-client"
  defaultClass: false
  reclaimPolicy: Delete
  mountOptions:
    - vers=4.1
  archiveOnDelete: "false"

rbac:
  create: true

# Opcional: configurar tolerations/nodeSelector si tu NFS client debe correr en nodos concretos
# nodeAffinity: {}
# tolerations: []
```


Ejemplo de PVC que reclama un volumen:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-dynamic
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: nfs-client   # StorageClass definida para el NFS Provisioner
  resources:
    requests:
      storage: 5Gi
```

## Consideraciones

Siempre que se use NFS (est√°tico o din√°mico) es necesario que todos los nodos donde pueden correr los Pods deben tener instalado el cliente NFS( NFS-common o nfs-utils).


## Aprovisionamiento distribuido

El dise√±o de kubernetes es din√°mico: los pods pueden moverse entre nodos, reiniciarse o escalar. Esto genera que pensemos como abordar el problema de la persistencia de datos aunque un Pod/nodo desaparezca. La soluci√≥n son los vol√∫menes persistentes (PV), que permiten desacoplar el ciclo de vida del contenedor del ciclo de vida de los datos. Pero si el almacenamiento est√° en un solo nodo, existe un punto √∫nico de fall√≥. Es por eso que existe el almacenamiento distribuido din√°mico.

### ¬ø Qu√© es almacenamiento distribuido?

Es un sistema que:
* Agrupa discos de varios nodos en un solo pool de almacenamiento.
* Replica o distribuye los datos entre nodos para evitar p√©rdida de informaci√≥n.
* Expone v√≥lumenes l√≥gicos que pueden montarse desde cualquier nodo del cl√∫ster.

Por defecto, kubernetes no trae un sistema de almacenamiento distribuido. Lo que hace es apoyarse en drivers CSI (Container Storage Interface) que permite integrar sistemas de almacenamiento externos.

Existen los siguientes sistemas:
- Longhorn
- Ceph
- GlusterFS
- Portworx

para:
- Aprovisionamiento din√°mico de vol√∫menes.
- Replicaci√≥n y resilencia de datos.
- Movilidad: Cualquier pod puede montar el mismo volumen desde otro nodo.

### Tipos de almacenamiento distribuido
 
* Almacenamiento basado en bloques (Rook-Ceph, Longhorn, OpenEBS)
    - Se presenta vol√∫menes tipo disco a los pods.
    - Ideales para base de datos y cargas de trabajo de alto rendimiento.
* Almacenamiento basado en archivos (CephFS, GlusterFS, NFS distribuido)
    - Permite que varios pods lean/escriban el mismo sistema de archivos.
    - √ötiles para aplicaciones que comparten datos.
* Almacenamiento basado en objetos (MinIO, Ceph Object, S3)
    - Los pods acceden a datos v√≠a API (HTTP/S3)
    - Bueno para backups, multimedia o big data.


### Beneficios

* Alta disponibilidad: Los datos sobreviven a la ca√≠da de nodos.
* Escalabilidad: Se agregan nodos/discos para crecer.
* Tolerancia a fallos: mediante replicaci√≥n o codificaci√≥n de borrado.
* Desacoplamiento de la infraestructura: los pods no dependen de un nodo f√≠sico en particular.

---

# Despliegue de almacenamiento distribuido Longhorn

Longhorn es un sistema de almacenamiento distribuido para Kubernetes que proporciona vol√∫menes persistentes mediante el uso de discos locales y almacenamiento en red. Permite la gesti√≥n sencilla de vol√∫menes, la replicaci√≥n de datos para alta disponibilidad, snapshots, backups y restauraciones. Longhorn es f√°cil de instalar y administrar, y est√° dise√±ado para integrarse de forma nativa con Kubernetes, facilitando la gesti√≥n del almacenamiento en cl√∫steres de contenedores.

## prerrequisitos

```bash
# En todos los nodos del cluster (prerrequisitos para longhorn)
sudo apt-get update
sudo apt-get install -y open-iscsi util-linux nfs-common

# Verificar que iscsid est√© corriendo
sudo systemctl enable iscsid
sudo systemctl start iscsid
```

validaci√≥n:

```bash
sudo modprobe nfs
sudo modprobe nfsd
which iscsiadm
which mount.nfs4
showmount --version
```
* open-iscsi ‚Üí Cliente iSCSI: Longhorn usa iSCSI para exponer vol√∫menes persistentes a los nodos. Sin este paquete, los pods no podr√≠an montar los discos de Longhorn.

* util-linux ‚Üí Conjunto de utilidades b√°sicas de Linux (ej. mount, fdisk, lsblk).: Necesarias para gestionar discos, montar vol√∫menes y trabajar con almacenamiento.

* nfs-common ‚Üí Cliente NFS para Linux: Longhorn lo necesita si vas a usar vol√∫menes RWX (ReadWriteMany) o hacer backups en NFS.

* modprobe nfs: Carga el m√≥dulo del kernel para que el cliente NFS funcione.

* modprobe nfsd: Carga el m√≥dulo del servidor NFS (necesario si un nodo act√∫a como servidor/exporta vol√∫menes).

* which iscsiadm: Verifica que la herramienta principal del cliente iSCSI (iscsiadm) est√° instalada. Es la que permite gestionar sesiones iSCSI.

* which mount.nfs4: Comprueba que el binario para montar sistemas de archivos NFSv4 est√° disponible. Necesario para RWX y backups en NFS.

* showmount --version: Confirma que el comando showmount (parte de nfs-common) est√° instalado y funcionando. Se usa para consultar exportaciones disponibles en un servidor NFS.

## Instalaci√≥n

1. Agregamos el repositorio de longhorn

```bash
helm repo add longhorn https://charts.longhorn.io
helm repo update
```

2. Creamos un namespace para longhorn

```bash
kubectl create namespace longhorn-system
```

3. Busca charts en un repositorio

```bash
helm search repo longhorn

NAME                    CHART VERSION   APP VERSION     DESCRIPTION
longhorn/longhorn       1.10.0          v1.10.0         Longhorn is a distributed block storage system ...
```

4. Busca charts en un ArtifactHub (cat√°logo p√∫blico)

```bash
helm search hub longhorn
```

5. Mostrar informaci√≥n del chart

```bash
helm show chart longhorn/longhorn

apiVersion: v1
appVersion: v1.10.0
description: Longhorn is a distributed block storage system for Kubernetes.
home: https://github.com/longhorn/longhorn
icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/longhorn/icon/color/longhorn-icon-color.png
keywords:
- longhorn
- storage
- distributed
- block
- device
- iscsi
- nfs
kubeVersion: '>=1.25.0-0'
maintainers:
- email: maintainers@longhorn.io
  name: Longhorn maintainers
name: longhorn
sources:
- https://github.com/longhorn/longhorn
- https://github.com/longhorn/longhorn-engine
- https://github.com/longhorn/longhorn-instance-manager
- https://github.com/longhorn/longhorn-share-manager
- https://github.com/longhorn/longhorn-manager
- https://github.com/longhorn/longhorn-ui
- https://github.com/longhorn/longhorn-tests
- https://github.com/longhorn/backing-image-manager
version: 1.10.0
```

6. Ver los valores configurables

```bash
helm show values longhorn/longhorn > defaultValues.yaml
```

7. Ver los templates/renderizado antes de instalar

```bash
helm template my-longhorn longhorn/longhorn --values defaultValues.yaml > my-longhorn-deploy.yaml
```

8. Instalaci√≥n de longhorn en su namespace correspondiente

```bash
helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --values values.yaml --debug --atomic --timeout 20m0s
```
* helm install: indica que se va a instalar un chart en kubernetes como un release.
* longhorn: es el nombre del release que le das en tu cl√∫ster. √∫til para luego realizar actualizaci√≥n o desinstalaci√≥n del release.
* longhorn/longhorn: es el chart a instalar; El primer longhorn es el nombre del repositorio de Helm, el segundo es el nombre del chart dentro de ese repo.
* --namespace longhorn-system: Instala todos los objetos (Deployments, DaemonSets, CRDs, Services, etc) en el namespace longhorn-system.
* --create-namespace: Si el namespace longhorn-system no existe, entonces lo crea previamente.
* --values values.yaml: Utiliza el archivo values.yaml personalizado en lugar de los valores por defecto. Se define valores para cambiar configuraciones de los objetos.
* --debug: Muestra informaci√≥n detallada en la consola: manifiestos renderizados, logs de instalaci√≥n y pasos de Helm.
* --atomic: Si algo falla durante la instalaci√≥n, Helm deshace todo automaticamente.
* --timeout 20m0s: Tiempo m√°ximo que helm esperar√° a que se creen todos los recursos. 


Mi DNS hacia mi cluster es k8scp entonces para conectarme a la interfaz de longhorn existen dos manera:
* port-forward: `kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80`
* a traves de un ingress & ingress controller. Para mi caso: `https://k8scp:30198/#/dashboard`


## ¬øPara qu√© sirven m√∫ltiples StorageClasses?

Cada aplicaci√≥n tiene **diferentes necesidades de almacenamiento**. Con m√∫ltiples StorageClasses puedes optimizar:

### 1. **Performance vs Redundancia**

```yaml
# StorageClass para BBDD cr√≠ticas (m√°xima redundancia)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-critical
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "3"        # 3 copias (alta disponibilidad)
  staleReplicaTimeout: "30"    # 30 min (detecta fallos r√°pido)
  diskSelector: "ssd"          # Solo usa discos SSD
  nodeSelector: "storage-tier:premium"
```

**RAZ√ìN**: Para PostgreSQL, MySQL, Elasticsearch ‚Üí m√°xima durabilidad

```yaml
# StorageClass para desarrollo/testing (velocidad)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-dev
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "1"        # 1 copia (m√°s r√°pido, menos espacio)
  staleReplicaTimeout: "1440"  # 24h (no importa si tarda)
```

**RAZ√ìN**: Para entornos ef√≠meros donde los datos no son cr√≠ticos

---

### 2. **Tipos de carga de trabajo**

```yaml
# Para logs/m√©tricas (escritura intensiva, lectura poco frecuente)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-logs
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "2"
  dataLocality: "best-effort"  # Prioriza escribir local (m√°s r√°pido)
  replicaAutoBalance: "least-effort"
```

**RAZ√ìN**: Para Loki, Prometheus, logging ‚Üí optimiza escritura

```yaml
# Para cach√© (puede perderse, lectura intensiva)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-cache
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "1"
  dataLocality: "strict-local"  # SOLO en nodo local (m√°xima velocidad)
```

**RAZ√ìN**: Para Redis, Memcached ‚Üí velocidad sobre durabilidad

---

### 3. **Backup y recuperaci√≥n**

```yaml
# Para vol√∫menes con backup autom√°tico
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-backed-up
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "2"
  recurringJobSelector: '[
    {"name":"backup-daily", "isGroup":true}
  ]'
```

**RAZ√ìN**: Aplica pol√≠ticas de backup autom√°ticas

---

### 4. **Separaci√≥n por tipo de disco**

```yaml
# Solo SSD NVMe (m√°ximo rendimiento)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-nvme
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "2"
  diskSelector: "nvme"
  nodeSelector: "storage-tier:nvme"
```

```yaml
# Solo HDD (gran capacidad, bajo costo)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-hdd
provisioner: driver.longhorn.io
parameters:
  numberOfReplicas: "2"
  diskSelector: "hdd"
  nodeSelector: "storage-tier:capacity"
```

**RAZ√ìN**: Para archivos grandes poco frecuentes (backups, archivos)

---

## Ejemplo Real: Arquitectura Completa

```yaml
# ============================================================================
# ARQUITECTURA DE STORAGE CLASSES
# ============================================================================

# 1. TIER PLATINUM - Aplicaciones cr√≠ticas
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-platinum
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain  # NO borra datos al eliminar PVC
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "30"
  diskSelector: "ssd,nvme"
  dataLocality: "disabled"  # Distribuye en diferentes nodos (HA)
  replicaAutoBalance: "best-effort"
  fsType: "ext4"
# USO: PostgreSQL producci√≥n, Elasticsearch, bases de datos cr√≠ticas

---
# 2. TIER GOLD - Aplicaciones importantes
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-gold
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  numberOfReplicas: "2"
  staleReplicaTimeout: "1440"
  diskSelector: "ssd"
  dataLocality: "best-effort"
  fsType: "ext4"
# USO: MySQL staging, MongoDB, Redis persistente

---
# 3. TIER SILVER - Desarrollo y testing
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-silver
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"  # Por defecto
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  numberOfReplicas: "2"
  staleReplicaTimeout: "2880"
  dataLocality: "best-effort"
  fsType: "ext4"
# USO: Default para la mayor√≠a de workloads

---
# 4. TIER BRONZE - Ef√≠mero/Cache
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-bronze
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  numberOfReplicas: "1"
  staleReplicaTimeout: "2880"
  dataLocality: "strict-local"  # Solo local (m√°xima velocidad)
  fsType: "ext4"
# USO: Cach√©, builds temporales, datos ef√≠meros

---
# 5. STORAGE ESPEC√çFICO - Archivos grandes
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-bulk
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
parameters:
  numberOfReplicas: "2"
  diskSelector: "hdd"
  fsType: "ext4"
# USO: Backups, archivos multimedia, datos hist√≥ricos
```

## üéØ Tabla de decisi√≥n: ¬øQu√© StorageClass usar?

| Aplicaci√≥n | StorageClass | Raz√≥n |
|------------|--------------|-------|
| **PostgreSQL Prod** | longhorn-platinum | Datos cr√≠ticos, 3 r√©plicas, SSD |
| **Elasticsearch** | longhorn-gold | I/O intensivo, 2 r√©plicas suficientes |
| **Redis persistente** | longhorn-gold | Importante pero puede reconstruirse |
| **MySQL Dev** | longhorn-silver | No cr√≠tico, 2 r√©plicas OK |
| **Redis cache** | longhorn-bronze | Puede perderse, 1 r√©plica local |
| **Prometheus** | longhorn-silver | Datos de m√©tricas, retenci√≥n corta |
| **MinIO/Object Storage** | longhorn-bulk | Archivos grandes, HDD m√°s econ√≥mico |
| **Jenkins builds** | longhorn-bronze | Datos temporales, velocidad > durabilidad |
| **Backups** | longhorn-bulk | Gran capacidad, acceso infrecuente |

## Beneficios de esta estrategia

### **1. Performance adecuado**
- BBDD ‚Üí baja latencia (SSD, local)
- Logs ‚Üí alta escritura (optimizado)
- Cache ‚Üí m√°xima velocidad (strict-local)

### **2. Gesti√≥n simplificada**
```bash
# Ver qu√© usa cada aplicaci√≥n
kubectl get pvc --all-namespaces -o custom-columns=\
NAME:.metadata.name,\
NAMESPACE:.metadata.namespace,\
STORAGECLASS:.spec.storageClassName,\
SIZE:.spec.resources.requests.storage
```

### **3. Troubleshooting f√°cil**
```bash
# "Esta BBDD va lenta"
# Revisas: ¬øEst√° usando longhorn-platinum o longhorn-bronze?
# Si usa bronze ‚Üí migra a platinum
```

## Par√°metros importantes de Longhorn

```yaml
parameters:
  # R√©plicas
  numberOfReplicas: "3"              # Cu√°ntas copias (1-3)
  
  # Localidad de datos
  dataLocality: "disabled"           # Distribuido (HA)
  dataLocality: "best-effort"        # Prefiere local pero permite remoto
  dataLocality: "strict-local"       # SOLO local (m√°s r√°pido, menos HA)
  
  # Selecci√≥n de discos/nodos
  diskSelector: "ssd,nvme"           # Tags de discos a usar
  nodeSelector: "storage-tier:premium"  # Tags de nodos a usar
  
  # Backup
  recurringJobSelector: '[{"name":"backup-daily"}]'
  
  # Comportamiento
  staleReplicaTimeout: "30"          # Minutos antes de marcar r√©plica como obsoleta
  replicaAutoBalance: "best-effort"  # Rebalancea r√©plicas autom√°ticamente
  
  # Sistema de archivos
  fsType: "ext4"                     # ext4 o xfs
  
  # Acceso
  migratable: "true"                 # Permite live migration
```

> Nota: Longhorn no sabe que disco es ssd o hhdd. Es necesario etiquetar los volumenes de los nodos por la interfaz de longhorn √≥ mediante el CRD de la configuraci√≥n del nodo creado por longhorn. 

[‚¨ÖÔ∏è Anterior](../kubernetes-install/install.md) | [üè† Volver al Inicio](../README.md) | [‚û°Ô∏è Siguiente](../postgres/postgres.md)


