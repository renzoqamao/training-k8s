# InstalaciÃ³n de elasticSearch en kubernetes

## Â¿QuÃ© son los operadores?

En k8s, un operador es un controlador especializado que extiende la API del clÃºster para gestionar aplicaciones complejas (instalar, escalar, actualizar, hacer copias de seguridad, recuperar de fallos), usando las mismas declaraciones primitivas de k8s.

### Â¿QuÃ© es?

* Un nuevo tipo de recurso: define un CRD (CustomResourceDefinition) como, por ejemplo, `Elasticsearch`, `Kafka`, `Postgres`.
* Controlador: un proceso que observa esos recursos y ejecuta la lÃ³gica para llevar el sistema al `estado deseado` (patrÃ³n `reconciliation loop`).
* Cerebro de dominio: codifica el conocimiento operativo de un SRE humano (procedimiento, Ã³rdenes de actualizaciÃ³n, salud, backups).

## Proceso de despliegue de Elasticsearch

### Paso 1: InstalaciÃ³n del operador ECK

Se instala en dos fases: CRDs primero, luego el operador

#### 1.1 Custom Resource Definitions (CRDs)

Define los nuevos "tipos" de objetos que ECK introduce en Kubernetes, estos son: Elasticsearch, Kibana, ApmServer, EnterpriseSearch, Beats, ElasticMapsServer, Agent, Logstash. Sin estos CRDs, Kubernetes no sabrÃ­a quÃ© hacer con un objeto tipo "kind: Elasticsearch"

```bash
kubectl create -f https://download.elastic.co/downloads/eck/2.10.0/crds.yaml
```
Â¿QuÃ© hace internamente?
- Registra nuevos tipos de recursos en la API de Kubernetes
- Define la estructura (schema) que deben tener estos recursos
- Permite validaciÃ³n automÃ¡tica de los manifiestos
- Debe instalarse ANTES del operador

#### 1.2 El Operador ECK

Es el "cerebro" que observa los recursos Elasticsearch/Kibana y los materializa. Un operador es un patrÃ³n de Kubernetes que extiende la funcionalidad de la API

```bash
kubectl apply -f https://download.elastic.co/downloads/eck/2.10.0/operator.yaml
```

Â¿QuÃ© contiene este manifiesto? (Principales componentes):
1. Namespace "elastic-system" - donde vive el operador
2. ServiceAccount - para que el operador pueda leer/escribir en K8s API
3. ClusterRole - permisos que necesita (crear pods, services, secrets, etc.)
4. ClusterRoleBinding - conecta el ServiceAccount con los permisos
5. StatefulSet/Deployment - el operador en sÃ­ (un pod que corre 24/7)
6. Service - para webhooks de validaciÃ³n
7. ValidatingWebhookConfiguration - valida tus manifiestos antes de aplicarlos

El operador entra en un loop infinito:
- Observa cambios en recursos tipo "Elasticsearch"
- Compara el estado actual vs. el deseado
- Toma acciones para reconciliar (crear pods, configurar networking, etc.)


### Paso 2: DefiniciÃ³n del namespace

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: elasticsearch
  labels:
    name: elasticsearch
```

- AÃ­sla los recursos de Elasticsearch
- Facilita gestiÃ³n con kubectl (kubectl get all -n elasticsearch)
- Permite aplicar polÃ­ticas de seguridad especÃ­ficas
- Mejor organizaciÃ³n en clusters multi-tenant

### Paso 3: DefiniciÃ³n del storageClass


```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-elasticsearch
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "2"
  staleReplicaTimeout: "2880"
  fromBackup: ""
  fsType: "ext4"
```
- Crea una StorageClass especÃ­fica para Elasticsearch
- numberOfReplicas: "2" - Longhorn mantiene 2 copias de los datos (HA)
- allowVolumeExpansion: true - Permite crecer los discos sin downtime
- staleReplicaTimeout: 48h - tiempo antes de marcar rÃ©plica como obsoleta
- fsType: ext4 - sistema de archivos (mejor rendimiento para ES que xfs en este caso)

#### Â¿Por quÃ© una StorageClass separada?

- Permite tuning especÃ­fico para cargas de trabajo de Elasticsearch
- Facilita troubleshooting (sabes quÃ© volÃºmenes son de ES)
- Puedes cambiar parÃ¡metros sin afectar otras apps

### Paso 4: DefiniciÃ³n del clÃºster de elasticSearch (CRD creado)
```yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch
  namespace: elasticsearch
# RAZÃ“N del metadata:
# - name: serÃ¡ el prefijo de todos los recursos (pods, services, secrets)
# - namespace: dÃ³nde vivirÃ¡ todo

spec:
  version: 8.11.0
  # RAZÃ“N: VersiÃ³n de Elasticsearch a desplegar
  # ECK se encarga de:
  # - Descargar la imagen correcta
  # - Aplicar configuraciones especÃ­ficas de esa versiÃ³n
  # - Gestionar migraciones si actualizas la versiÃ³n
  
  # --------------------------------------------------------------------------
  # HTTP Configuration
  # --------------------------------------------------------------------------
  http:
    service:
      spec:
        type: ClusterIP
        # RAZÃ“N del tipo ClusterIP:
        # - Solo accesible dentro del cluster (mÃ¡s seguro)
        # - Para acceso externo usarÃ­as: LoadBalancer o NodePort
        # - En local, usarÃ¡s port-forward para acceder
    tls:
      selfSignedCertificate:
        disabled: true
        # RAZÃ“N disabled: true:
        # - Simplifica desarrollo local (no necesitas gestionar certificados)
        # - En producciÃ³n deberÃ­as: disabled: false (TLS habilitado)
        # - ECK auto-genera certificados si estÃ¡ habilitado
  
  # --------------------------------------------------------------------------
  # NodeSets - Define grupos de nodos con roles especÃ­ficos
  # --------------------------------------------------------------------------
  nodeSets:
  - name: master-data
    # RAZÃ“N del nombre:
    # - Identificador del grupo de nodos
    # - Los pods se llamarÃ¡n: elasticsearch-es-master-data-0, -1, -2
    # - Puedes tener mÃºltiples nodeSets con roles diferentes
    
    count: 3
    # RAZÃ“N count: 3:
    # - MÃ­nimo recomendado para evitar split-brain
    # - Con 3 nodos master, el quorum es 2 (mayorÃ­a)
    # - Si un nodo cae, el cluster sigue funcionando
    
    config:
      # ConfiguraciÃ³n de Elasticsearch que irÃ¡ en elasticsearch.yml
      node.roles: ["master", "data", "ingest"]
      # RAZÃ“N de los roles:
      # - master: Puede ser elegido como master del cluster (gestiona metadata)
      # - data: Almacena datos e Ã­ndices
      # - ingest: Puede procesar pipelines de ingest (transformaciones)
      # 
      # Nodos hÃ­bridos (master+data) son buenos para:
      # - Clusters pequeÃ±os/medianos (como el tuyo)
      # - Simplifica arquitectura
      # En clusters grandes separarÃ­as: nodos master dedicados, data nodes, etc.
      
      node.store.allow_mmap: false
      # RAZÃ“N:
      # - Desactiva memory-mapped files
      # - Ãštil en entornos con restricciones de memoria
      # - Puede reducir rendimiento pero aumenta estabilidad en VMs
      
      xpack.security.enabled: true
      # RAZÃ“N:
      # - Habilita autenticaciÃ³n (usuario/password)
      # - ECK auto-genera credenciales y las guarda en un Secret
      # - Para obtener la password: 
      #   kubectl get secret elasticsearch-es-elastic-user -n elasticsearch -o=jsonpath='{.data.elastic}' | base64 --decode
      
      xpack.security.transport.ssl.enabled: true
      # RAZÃ“N:
      # - Encripta comunicaciÃ³n entre nodos del cluster
      # - Obligatorio cuando xpack.security.enabled: true
      # - ECK gestiona los certificados automÃ¡ticamente
      
      xpack.security.http.ssl.enabled: false
      # RAZÃ“N:
      # - Desactiva HTTPS en la API REST (simplifica desarrollo local)
      # - En producciÃ³n deberÃ­as usar: true
      # - Si es true, accederÃ­as con https://localhost:9200
    
    # ------------------------------------------------------------------------
    # PodTemplate - Personaliza los pods de Elasticsearch
    # ------------------------------------------------------------------------
    podTemplate:
      metadata:
        labels:
          app: elasticsearch
          # RAZÃ“N:
          # - Facilita selecciÃ³n con kubectl (kubectl get pods -l app=elasticsearch)
          # - Ãštil para NetworkPolicies, Services, etc.
      
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - worker01
        # ----------------------------------------------------------------------
        # InitContainers - Se ejecutan ANTES del contenedor principal
        # ----------------------------------------------------------------------
        initContainers:
        - name: sysctl
          # RAZÃ“N de este initContainer:
          # - Elasticsearch requiere vm.max_map_count >= 262144
          # - Es un parÃ¡metro del kernel del host
          # - Solo se puede cambiar con privilegios
          securityContext:
            privileged: true
            # RAZÃ“N privileged: true:
            # - Permite modificar parÃ¡metros del kernel
            # - Solo se usa en initContainer (no en el contenedor principal)
            # - Necesario para sysctl
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
          # Â¿QuÃ© hace?
          # - sysctl modifica parÃ¡metros del kernel
          # - vm.max_map_count: nÃºmero mÃ¡ximo de Ã¡reas de memoria mapeadas
          # - Elasticsearch usa mmap para Ã­ndices (mejor rendimiento)
          # - Sin esto, Elasticsearch no arranca
        
        - name: install-plugins
          # RAZÃ“N (OPCIONAL):
          # - Instala plugins de Elasticsearch antes de iniciar
          # - Ejemplo: analysis-icu para soporte Unicode avanzado
          # - Se ejecuta una vez al crear el pod
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin install --batch analysis-icu
          # Plugins Ãºtiles:
          # - analysis-icu: AnÃ¡lisis de texto Unicode
          # - repository-s3: Snapshots en AWS S3
          # - discovery-ec2: Discovery en AWS
        
        # ----------------------------------------------------------------------
        # Containers - El contenedor principal de Elasticsearch
        # ----------------------------------------------------------------------
        containers:
        - name: elasticsearch
          # RAZÃ“N del nombre:
          # - ECK espera este nombre especÃ­fico
          # - No lo cambies o ECK no funcionarÃ¡ correctamente
          
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms2g -Xmx2g"
            # RAZÃ“N:
            # - Xms: Heap inicial de Java
            # - Xmx: Heap mÃ¡ximo de Java
            # - Deben ser IGUALES (evita resize de heap = mejor rendimiento)
            # - Regla: 50% de la RAM del pod, mÃ¡ximo 31GB
            # - Para 4GB de RAM del pod â†’ 2GB de heap
            # 
            # âš ï¸ IMPORTANTE:
            # - Nunca mÃ¡s de 31GB (compressed oops de JVM)
            # - El resto de RAM es para: filesystem cache, Lucene, OS
          
          - name: READINESS_PROBE_TIMEOUT
            value: "10"
            # RAZÃ“N:
            # - Timeout para el readiness probe
            # - Si el cluster es lento al iniciar, aumenta esto
            # - Evita que K8s mate pods prematuramente
          
          resources:
            requests:
              memory: 2Gi
              cpu: 1
              # RAZÃ“N de requests:
              # - Garantiza estos recursos mÃ­nimos
              # - K8s solo programa el pod si hay recursos disponibles
              # - Afecta scheduling (dÃ³nde se coloca el pod)
            
            limits:
              memory: 3Gi
              cpu: 2
              # RAZÃ“N de limits:
              # - memory: DEBE ser igual a request para ES (evita OOM kills)
              # - cpu: Puede ser mayor (permite burst)
              # - Si se excede memory, K8s mata el pod (OOMKilled)
          
          # --------------------------------------------------------------------
          # Probes - Salud del contenedor
          # --------------------------------------------------------------------
          # RAZÃ“N de las probes:
          # - livenessProbe: Â¿El proceso estÃ¡ vivo? Si falla â†’ restart
          # - readinessProbe: Â¿EstÃ¡ listo para trÃ¡fico? Si falla â†’ quita de Service
          # - ECK configura estas automÃ¡ticamente, pero podemos ajustarlas
    
    # ------------------------------------------------------------------------
    # VolumeClaimTemplates - Almacenamiento persistente
    # ------------------------------------------------------------------------
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
        # RAZÃ“N:
        # - Nombre del volumen dentro del pod
        # - Se monta en /usr/share/elasticsearch/data (path por defecto)
      
      spec:
        accessModes:
        - ReadWriteOnce
        # RAZÃ“N ReadWriteOnce:
        # - Solo un nodo puede montar el volumen a la vez
        # - Suficiente para StatefulSets (cada pod su volumen)
        # - Alternativas:
        #   - ReadWriteMany: mÃºltiples nodos (no lo necesitas con StatefulSets)
        #   - ReadOnlyMany: solo lectura desde mÃºltiples nodos
        
        resources:
          requests:
            storage: 30Gi
        # RAZÃ“N 30Gi:
        # - Depende de cuÃ¡ntos datos esperas indexar
        # - Elasticsearch recomienda 80% de uso mÃ¡ximo
        # - Puedes expandir despuÃ©s si allowVolumeExpansion: true
        # - Para producciÃ³n: calcula basado en:
        #   - Volumen diario de datos
        #   - RetenciÃ³n (cuÃ¡ntos dÃ­as de datos)
        #   - NÃºmero de rÃ©plicas de Ã­ndices
        
        storageClassName: longhorn-elasticsearch
        # RAZÃ“N:
        # - Usa la StorageClass que creamos antes
        # - Longhorn provisionarÃ¡ el volumen automÃ¡ticamente
        # - Cada pod (0,1,2) tendrÃ¡ su propio PVC/PV

```
### Paso 5: DefiniciÃ³n de kibana (Opcional)

```yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana
  namespace: elasticsearch
# RAZÃ“N de Kibana:
# - Interface grÃ¡fica para Elasticsearch
# - VisualizaciÃ³n de datos
# - Dev Tools para queries
# - GestiÃ³n de Ã­ndices, snapshots, etc.
# - Monitoreo del cluster

spec:
  version: 8.11.0
  # RAZÃ“N: Debe coincidir con la versiÃ³n de Elasticsearch
  
  count: 1
  # RAZÃ“N count: 1:
  # - Kibana es stateless (no almacena datos)
  # - 1 instancia es suficiente para desarrollo
  # - En producciÃ³n podrÃ­as usar 2+ para HA
  
  elasticsearchRef:
    name: elasticsearch
    # RAZÃ“N:
    # - Conecta Kibana con el Elasticsearch del mismo namespace
    # - ECK auto-configura:
    #   - URL de conexiÃ³n
    #   - Credenciales (usa el usuario elastic)
    #   - Certificados TLS si estÃ¡n habilitados
    # - Si estuviera en otro namespace: 
    #   name: elasticsearch
    #   namespace: otro-namespace
  
  http:
    service:
      spec:
        type: ClusterIP
        # RAZÃ“N ClusterIP:
        # - Acceso interno al cluster
        # - UsarÃ¡s port-forward: kubectl port-forward svc/kibana-kb-http 5601 -n elasticsearch
        # - Para acceso externo: LoadBalancer o Ingress
    
    tls:
      selfSignedCertificate:
        disabled: true
        # RAZÃ“N disabled: true:
        # - Simplifica desarrollo (acceso HTTP simple)
        # - En producciÃ³n: false (HTTPS habilitado)
  
  podTemplate:
    spec:
      containers:
      - name: kibana
        resources:
          requests:
            memory: 1Gi
            cpu: 500m
            # RAZÃ“N:
            # - Kibana necesita menos recursos que Elasticsearch
            # - 1GB RAM suele ser suficiente para dev
            # - CPU: 0.5 cores request, puede usar mÃ¡s si estÃ¡ disponible
          limits:
            memory: 2Gi
            cpu: 2
            # RAZÃ“N:
            # - Permite burst cuando hay dashboards complejos
            # - 2GB lÃ­mite de memoria para evitar OOM
        
        env:
        - name: NODE_OPTIONS
          value: "--max-old-space-size=1024"
          # RAZÃ“N:
          # - Kibana corre en Node.js
          # - Limita heap de Node.js a 1GB
          # - Previene uso excesivo de memoria
          # - Debe ser menor que el lÃ­mite de memoria del container

```


### Paso 6: DefiniciÃ³n de politicas de red (Opcional)

- Limita quÃ© pods pueden comunicarse con Elasticsearch
- Solo Kibana y apps autorizadas
- Defensa en profundidad

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: elasticsearch-netpol
  namespace: elasticsearch
spec:
  podSelector:
    matchLabels:
      elasticsearch.k8s.elastic.co/cluster-name: elasticsearch
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          kibana.k8s.elastic.co/name: kibana
    # Permite trÃ¡fico desde Kibana
  - from:
    - namespaceSelector:
        matchLabels:
          name: mi-app
    # Permite trÃ¡fico desde namespace de tu aplicaciÃ³n
  - ports:
    - protocol: TCP
      port: 9300
    from:
    - podSelector:
        matchLabels:
          elasticsearch.k8s.elastic.co/cluster-name: elasticsearch
    # Permite comunicaciÃ³n entre nodos de ES (transport port)
```



## Despliegue

```bash
kubectl apply -f 01_ns.yaml
kubectl apply -f 02_sc-es.yaml
kubectl apply -f 03_crd-es.yaml
kubectl apply -f 04_crd-kibana.yaml
kubectl apply -f 05_np.yaml # (opcional)
```
## Acceso a ElasticSearch

ECK crea automÃ¡ticamente un usuario `elastic` con una contraseÃ±a aleatoria guardada en un secret:

```bash
kubectl get secret elasticsearch-es-elastic-user -n elasticsearch -o go-template='{{.data.elastic | base64decode}}' # C58LY24Hi8rbpR9269mNcke7
```

Creamos un port-forward

```bash
kubectl port-forward svc/elasticsearch-es-http 9200:9200 -n elasticsearch
```

Consumimos algunos endpoints:

```bash
# Probar conexiÃ³n
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" http://localhost:9200
# Ver salud
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" http://localhost:9200/_cluster/health?pretty
# Ver nodos
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" http://localhost:9200/_cat/nodes?v
# Crear un Ã­ndice de prueba
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" -X PUT http://localhost:9200/mi-indice-test
# Indexar un documento
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" -X POST http://localhost:9200/mi-indice-test/_doc -H 'Content-Type: application/json' -d '{"mensaje": "Hola mundo", "fecha": "2025-10-09"}'
# Buscar documentos
curl -u "elastic:C58LY24Hi8rbpR9269mNcke7" http://localhost:9200/mi-indice-test/_search?pretty
```

## Acceso a Kibana

1. Verificamos que kibana este corriendo:

```bash
 kubectl get kibana -n elasticsearch
NAME     HEALTH   NODES   VERSION   AGE
kibana   red              8.11.0    45m
 kubectl get pods -n elasticsearch -l common.k8s.elastic.co/type=kibana
NAME                         READY   STATUS    RESTARTS   AGE
kibana-kb-856cb875f6-7jnts   0/1     Running   0          15m
```

2. Exponemos kibana por Port-Forward

```bash
kubectl port-forward svc/kibana-kb-http 5601:5601 -n elasticsearch --address 127.0.0.1
```

3.  Ingresamos a `http://localhost:5601` y veremos el login de kibana

![login kibana](./img/login.png)

4. Ingresamos nuestras credenciales `username: elastic`, `password: C58LY24Hi8rbpR9269mNcke7`.

5. Veremos el home

![kibana home](./img/home.png)


6. Podemos ir a > Search > Content > Elasticsearch indices y veremos el indice creado previamente

![search content](./img/search-content.png)

![search in content](./img/search-in-content.png)

[â¬…ï¸ Anterior](../sidecar/sidecar.md) | [ğŸ  Volver al Inicio](../README.md)